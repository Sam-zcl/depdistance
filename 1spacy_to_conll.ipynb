{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turning text into a ConLL file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate dependency distance, a document has to be split into sentences, tokenized, and parsed. This can be done using [Spacy](http://spacy.io) or [UDPipe](https://ufal.mff.cuni.cz/udpipe). Spacy's main interface is in Python, so we'll use that. UDPipe has an interface to R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy and Python\n",
    "First load Spacy and the English language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load your document. This might come from a text file or database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I eat the pizza.\n",
      "The pizza, which I liked, was eaten by me.\n"
     ]
    }
   ],
   "source": [
    "text = \"I eat the pizza. The pizza, which I liked, was eaten by me.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy has successfully split the two sentences.\n",
    "\n",
    "In the [ConLL format](http://universaldependencies.org/format.html), each line represents a token, and a new sentence is indicated by a newline. Each line contains 10 fields, not all of which we will fill with Spacy. Empty fields are marked with an underscore (`_`). Before each sentence, lines starting with `#` contain metadata belonging to that sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# newdoc id = 1\n",
      "# sent_id = 1\n",
      "# text = I eat the pizza.\n",
      "1\tI\t-PRON-\tPRON\tPRP\t_\t2\tnsubj\t_\t_\n",
      "2\teat\teat\tVERB\tVBP\t_\t0\troot\t_\t_\n",
      "3\tthe\tthe\tDET\tDT\t_\t4\tdet\t_\t_\n",
      "4\tpizza\tpizza\tNOUN\tNN\t_\t2\tdobj\t_\t_\n",
      "5\t.\t.\tPUNCT\t.\t_\t2\tpunct\t_\t_\n",
      "\n",
      "# sent_id = 2\n",
      "# text = The pizza, which I liked, was eaten by me.\n",
      "1\tThe\tthe\tDET\tDT\t_\t2\tdet\t_\t_\n",
      "2\tpizza\tpizza\tNOUN\tNN\t_\t9\tnsubjpass\t_\t_\n",
      "3\t,\t,\tPUNCT\t,\t_\t2\tpunct\t_\t_\n",
      "4\twhich\twhich\tADJ\tWDT\t_\t6\tdobj\t_\t_\n",
      "5\tI\t-PRON-\tPRON\tPRP\t_\t6\tnsubj\t_\t_\n",
      "6\tliked\tlike\tVERB\tVBD\t_\t2\trelcl\t_\t_\n",
      "7\t,\t,\tPUNCT\t,\t_\t9\tpunct\t_\t_\n",
      "8\twas\tbe\tVERB\tVBD\t_\t9\tauxpass\t_\t_\n",
      "9\teaten\teat\tVERB\tVBN\t_\t0\troot\t_\t_\n",
      "10\tby\tby\tADP\tIN\t_\t9\tagent\t_\t_\n",
      "11\tme\t-PRON-\tPRON\tPRP\t_\t10\tpobj\t_\t_\n",
      "12\t.\t.\tPUNCT\t.\t_\t9\tpunct\t_\t_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def to_conll(docs, docnames=None):\n",
    "    \"\"\" docs is a list of document already parsed with spacy.\n",
    "        docnames is optional, and contains the relevant document IDs.\"\"\"\n",
    "    # use numeric ids if not docnames are given\n",
    "    if not docnames:\n",
    "        docnames = list(range(1, len(docs)+1))\n",
    "    assert(len(docs)==len(docnames))\n",
    "    # this holds the conll output\n",
    "    conll = []\n",
    "    # iterate over documents\n",
    "    for i_doc, doc in enumerate(docs):\n",
    "        conll.append('# newdoc id = {}'.format(docnames[i_doc]))\n",
    "        # iterate over sentences\n",
    "        for i_sent, words in enumerate(doc.sents):\n",
    "            conll.append(\"# sent_id = {}\".format(i_sent+1))\n",
    "            conll.append(\"# text = {}\".format(str(words)))\n",
    "            # iterate over words\n",
    "            for i, word in enumerate(words):\n",
    "                if word.head == word:\n",
    "                    head = 0\n",
    "                else:\n",
    "                    # head should refer to within-document id\n",
    "                    head = word.head.i - words[0].i + 1\n",
    "                line = \"{id}\\t{form}\\t{lemma}\\t{upos}\\t{xpos}\\t_\\t{head}\\t{dep}\\t_\\t_\".\\\n",
    "                    format(id=i+1, form=word, lemma=word.lemma_ or \"_\", \n",
    "                           upos=word.pos_ or \"_\", xpos=word.tag_ or \"_\",\n",
    "                           head=head, dep=word.dep_.lower() or \"_\")\n",
    "                conll.append(line)\n",
    "            conll.append(\"\") # sentency boundary\n",
    "    return(\"\\n\".join(conll))\n",
    "\n",
    "print(to_conll([doc]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this method can now be saved into a `.conll` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDPipe and R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R interface to UDPipe makes it very easy to parse a list of documents and export it automatically to a ConLL file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n"
     ]
    }
   ],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    doc_id paragraph_id sentence_id                                   sentence\n",
       "1  My text            1           1                           I eat the pizza.\n",
       "2  My text            1           1                           I eat the pizza.\n",
       "3  My text            1           1                           I eat the pizza.\n",
       "4  My text            1           1                           I eat the pizza.\n",
       "5  My text            1           1                           I eat the pizza.\n",
       "6  My text            1           2 The pizza, which I liked, was eaten by me.\n",
       "7  My text            1           2 The pizza, which I liked, was eaten by me.\n",
       "8  My text            1           2 The pizza, which I liked, was eaten by me.\n",
       "9  My text            1           2 The pizza, which I liked, was eaten by me.\n",
       "10 My text            1           2 The pizza, which I liked, was eaten by me.\n",
       "11 My text            1           2 The pizza, which I liked, was eaten by me.\n",
       "12 My text            1           2 The pizza, which I liked, was eaten by me.\n",
       "13 My text            1           2 The pizza, which I liked, was eaten by me.\n",
       "14 My text            1           2 The pizza, which I liked, was eaten by me.\n",
       "15 My text            1           2 The pizza, which I liked, was eaten by me.\n",
       "16 My text            1           2 The pizza, which I liked, was eaten by me.\n",
       "17 My text            1           2 The pizza, which I liked, was eaten by me.\n",
       "   token_id token lemma  upos xpos\n",
       "1         1     I     I  PRON  PRP\n",
       "2         2   eat   eat  VERB  VBP\n",
       "3         3   the   the   DET   DT\n",
       "4         4 pizza pizza  NOUN   NN\n",
       "5         5     .     . PUNCT    .\n",
       "6         1   The   the   DET   DT\n",
       "7         2 pizza pizza  NOUN   NN\n",
       "8         3     ,     , PUNCT    ,\n",
       "9         4 which which  PRON  WDT\n",
       "10        5     I     I  PRON  PRP\n",
       "11        6 liked  like  VERB  VBD\n",
       "12        7     ,     , PUNCT    ,\n",
       "13        8   was    be   AUX  VBD\n",
       "14        9 eaten   eat  VERB  VBN\n",
       "15       10    by    by   ADP   IN\n",
       "16       11    me     I  PRON  PRP\n",
       "17       12     .     . PUNCT    .\n",
       "                                                   feats head_token_id\n",
       "1             Case=Nom|Number=Sing|Person=1|PronType=Prs             2\n",
       "2                       Mood=Ind|Tense=Pres|VerbForm=Fin             0\n",
       "3                              Definite=Def|PronType=Art             4\n",
       "4                                            Number=Sing             2\n",
       "5                                                   <NA>             2\n",
       "6                              Definite=Def|PronType=Art             2\n",
       "7                                            Number=Sing             9\n",
       "8                                                   <NA>             2\n",
       "9                                           PronType=Rel             6\n",
       "10            Case=Nom|Number=Sing|Person=1|PronType=Prs             6\n",
       "11                      Mood=Ind|Tense=Past|VerbForm=Fin             2\n",
       "12                                                  <NA>             9\n",
       "13 Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin             9\n",
       "14                   Tense=Past|VerbForm=Part|Voice=Pass             0\n",
       "15                                                  <NA>            11\n",
       "16            Case=Acc|Number=Sing|Person=1|PronType=Prs             9\n",
       "17                                                  <NA>             9\n",
       "      dep_rel deps            misc\n",
       "1       nsubj <NA>            <NA>\n",
       "2        root <NA>            <NA>\n",
       "3         det <NA>            <NA>\n",
       "4         obj <NA>   SpaceAfter=No\n",
       "5       punct <NA>            <NA>\n",
       "6         det <NA>            <NA>\n",
       "7  nsubj:pass <NA>   SpaceAfter=No\n",
       "8       punct <NA>            <NA>\n",
       "9         obj <NA>            <NA>\n",
       "10      nsubj <NA>            <NA>\n",
       "11  acl:relcl <NA>   SpaceAfter=No\n",
       "12      punct <NA>            <NA>\n",
       "13   aux:pass <NA>            <NA>\n",
       "14       root <NA>            <NA>\n",
       "15       case <NA>            <NA>\n",
       "16        obl <NA>   SpaceAfter=No\n",
       "17      punct <NA> SpacesAfter=\\\\n\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R -i text\n",
    "library(udpipe)\n",
    "# download the model using udpipe_download_model\n",
    "udmodel_english <- udpipe_load_model(file = \"~/english-ud-2.0-170801.udpipe\")\n",
    "\n",
    "# if you parse several texts at once, use doc_id to keep track\n",
    "# of each document\n",
    "doc <- udpipe_annotate(udmodel_english, x = text, doc_id = c(\"My text\"))\n",
    "print(as.data.frame(doc))\n",
    "\n",
    "# to export, use:\n",
    "cat(doc$conllu, file = \"annotated.conll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
